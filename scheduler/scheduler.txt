https://medium.com/@hatronix/inside-the-go-scheduler-a-step-by-step-look-at-goroutine-management-1a8cbe9d5dbd


Inside the Go Scheduler: A Step-by-Step Look at Goroutine Management

Step 1: Creating a New Goroutine
    The journey begins with the creation of a new goroutine. Goroutines are lightweight 
    threads of execution in Go and are your primary tool for concurrent programming. 
    They’re spawned with ease, allowing you to perform tasks in parallel without much effort.


Step 2: Managing Goroutines in Queues
    Now, let’s dive into the details of how goroutines are managed. When a new goroutine is 
    created, it enters the scheduler. Here’s what happens: If there’s enough space in the local 
    queue, the goroutine is placed there. Each OS thread (M) has its local queue for goroutines. 
    If the local queue is full, the goroutine is placed into a global queue. This global queue 
    is accessible to all OS threads (M), allowing any of them to pull goroutines from it to
    execute. When a thread is blocked in a system call (for example, during I/O operations), 
    it doesn’t need to maintain its local run queue. In this case, the scheduler ensures that 
    these goroutines are executed elsewhere, maintaining system responsiveness


Step 3: Executing Goroutines on OS Threads
    This step is all about ensuring that goroutines are executed on the available 
    OS threads (M): Each goroutine (G) must be executed on an OS thread (M).
    The relationship between M and P (processors) is one-to-one. If there’s a goroutine 
    that can be executed in a processor bound by an OS thread (M), it’s pulled from the 
    processor’s local queue to execute. If a processor (P) is empty and there are no executable
    goroutines, the OS thread (M) pulls a goroutine from the global queue. If the global 
    queue is also empty, the scheduler looks to pull goroutines from other processors.


Step 4: Allocating Resources
    Before a goroutine can run, it needs the necessary resources allocated to it. 
    This includes things like memory and stack space. The scheduler ensures that these resources 
    are prepared, and the goroutine is ready to roll.


Step 5: Execution on the CPU
    Finally, the CPU is allocated to the goroutine, and it starts executing the function. 
    The number of processors is determined by the maximum number of GOMAXPROCS, which sets 
    the limit for parallelism.


Step 6. Network Go: Handling Asynchronous System Calls
    The Go runtime includes a Network Poller component to manage asynchronous system calls, 
    such as network I/O: When a goroutine is waiting for a network request to complete, 
    it’s added to the network poller to avoid blocking a kernel thread. If both the global 
    and local run queues are empty, the processor polls the network. If a runnable goroutine 
    is found in the network poller, it’s added to the local run queue. In cases where 
    everything is empty and the network poller is blocked, the processor may randomly 
    steal work from another processor, including itself.



Some specific cases:
    A. Processor Work Stealing: Load Balancing in Go
        The Go scheduler employs a clever technique known as work stealing to balance the 
        load among processor’s kernel threads. Before a processor starts stealing work, 
        it checks its local run queue. If it’s not empty, the processor grabs a goroutine from 
        there and executes it. When the local run queue is empty, it then checks the global run 
        queue but does so only 1/61 of the time to maintain fairness. The stealing process is 
        designed to be fair, ensuring that each processor steals an equivalent number of 
        goroutines based on the number of processors. For example, with 3 processors and 6 
        goroutines in the global run queue, each processor takes 2 goroutines. When a processor 
        with runnable goroutines is found, it grabs half of them and adds them to its local 
        run queue, ensuring efficient distribution of work.


    B. Handoff: Dealing with Blocking Syscalls
        What happens when a goroutine, which usually runs on a kernel thread, makes a 
        blocking syscall? Here’s where the concept of handoff comes into play: If a goroutine 
        makes a system call, it blocks the kernel thread, potentially starving other goroutines 
        in the local run queue. To address this, the Go runtime invokes “releasep,” 
        which disassociates the processor from the blocked kernel thread. The processor is 
        then assigned a new kernel thread that’s either readily available or created as needed,
        a process known as handoff. Handoffs can be costly, especially when creating new 
        kernel threads. Notably, performing a handoff for every syscall may not be optimal, 
        as some syscalls are short-lived, and the kernel thread may not remain blocked for long.

    C. Optimizing Handoff
        The Go scheduler is smart about handoffs and optimizes them for efficiency. 
        Immediate handoff occurs when the runtime knows that a syscall will be blocking 
        for an extended period, like reading from a socket. In other cases, the processor 
        remains blocked, but its status is set to indicate it’s in a syscall. The runtime 
        periodically checks using “Sysmon” if the processor is still in a syscall. If it is, 
        a handoff is initiated.

    D. Returning from a Handed-Off Syscall
        When a goroutine returns from a syscall, the scheduler determines how to proceed: If the old 
        processor (the one that was handed off) is available, the goroutine associates itself with it. 
        If the old processor is unavailable, the goroutine joins an idle processor. If no idle 
        processors are available, the goroutine joins the global run queue, with parked goroutines that
        were in the syscall.